---
title: "Statistics with R"
subtitle: "Introduction to R for Actuarial Students"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    lib_dir: libs
    nature:
      highlightStyle: github
      countIncrementalSlides: false
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(magrittr)
library(knitr)
library(dplyr)
library(janitor)


```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_accent_inverse(
  #base_color = "#3C989E")(
  base_color = "#1c5253",
  header_font_google = google_font("Josefin Sans"),
  text_font_google   = google_font("Montserrat", "300", "300i"),
  code_font_google   = google_font("Fira Mono")
)
```


* Introduction to R for Actuarial Students

* CS1B Curriculum

* Introduction to R programming
* Fundamentals of Statistical Analysis

* Question 1
---

                                                                                                                                       
CS2B-1124 
Q. 1)  
install.packages("dplyr", type = "binary") 
install.packages("randomForest", type = "binary") 
install.packages("caret", type = "binary") 
install.packages("rpart", type = "binary") 
install.packages("rpart.plot", type = "binary") 
library(dplyr) 
library(randomForest)  
library(caret) 
library(rpart) 
library(rpart.plot) 
i) 
heart_disease <- read.csv("<path>/heart_disease.csv") 
heart_disease$sex <- as.factor(heart_disease$sex) 
heart_disease$cp <- as.factor(heart_disease$cp) 
heart_disease$fbs <- as.factor(heart_disease$fbs) 
heart_disease$restecg <- as.factor(heart_disease$restecg) 
heart_disease$exang <- as.factor(heart_disease$exang) 
heart_disease$slope <- as.factor(heart_disease$slope) 
heart_disease$ca <- as.factor(heart_disease$ca) 
heart_disease$thal <- as.factor(heart_disease$thal) 
heart_disease$target <- as.factor(heart_disease$target) 
Alternate Solution 
ii) 
heart_disease <- heart_disease %>% mutate(across(c(sex, cp, fbs, restecg, exang, slope, ca, 
thal, target), factor)) 
(3) 
train_index <- c(1:(nrow(heart_disease)*0.7)) 
train_data <- heart_disease[train_index, ] 
test_data <- heart_disease[-train_index, ] 
iii)  
set.seed(123) 
(2) 
tree_model <- rpart(target ~ age + sex + cp + trestbps + chol + fbs + restecg + thalach + exang 
+ oldpeak + slope + ca + thal,data = train_data, method = "class") 
rpart.plot(tree_model) 
Page 2 of 19 
IAI                                                                                                                                                  
CS2B-1124 
Inferences 
• Patients have been split basis the value of thalassemia. If the value is less than 3, then 
they go to left node else to the right node. Thus, 45% of the data has moved to right 
node and remaining to the left node. 
iv)  
• On the left node, a further split has been done basis the age of the patient. If the age  is 
less than 59, then it has branched leftward. This represents 35% of the total data. Thus, 
the probability that a heart disease is diagnosed when thalassemia is 3 and age of the 
patient is less than 59 is 0.08. However, with thalassemia being 3, age being greater 
than 59 and chest pain with 4, the probability of diagnosis the heart diseases is 0.73 
• The right node has been split basis the value of the chest pain. If the value is less than 
or equal to 3, it has been further split basis the number of major vessels. 
• Thus, if the thalassemia is not normal but, chest pain being less than equal to 3 and 
number of colored vessels being 0, the probability of getting heart diseases diagnosed 
is 0.24. The probability increases to 0.75 if number of colored vessels is greater than 0. 
• However, if thalassemia is not normal and chest pain is 4, the probability of getting 
heart diseases diagnosed increases to 0.89. 31% of total population falls here. 
(3) 
pred_tree <- predict(tree_model, test_data, type = "class") 
confusionMatrix(pred_tree, test_data$target) 
Confusion Matrix and Statistics 
Reference 
Prediction  0  1 
0 39 14 
1  7 30 
Accuracy : 0.7667           
95% CI : (0.6657, 0.8494) 
No Information Rate : 0.5111           
Page 3 of 19 
IAI                                                                                                                                                  CS2B-1124 
Page 4 of 19 
 
    P-Value [Acc > NIR] : 5.619e-07        
                                           
                  Kappa : 0.5315           
                                           
 Mcnemar's Test P-Value : 0.1904           
                                           
            Sensitivity : 0.8478           
            Specificity : 0.6818           
         Pos Pred Value : 0.7358           
         Neg Pred Value : 0.8108           
             Prevalence : 0.5111           
         Detection Rate : 0.4333           
   Detection Prevalence : 0.5889           
      Balanced Accuracy : 0.7648           
                                           
       'Positive' Class : 0            
 
confusion_matrix_tree <- confusionMatrix(pred_tree, test_data$target) 
 
confusion_matrix_tree$overall['Accuracy'] 
 Accuracy  
0.7666667   
 
confusion_matrix_tree$byClass['Precision'] 
Precision  
0.7358491 
 
confusion_matrix_tree$byClass['Recall'] 
   Recall  
0.8478261  
(4) 
###########################################################################################

Here’s a tidied-up version of your code and analysis. I've structured it for improved readability:

---

### CS2B-1124: Question 1

#### Setup
```r
# Install required packages
install.packages(c("dplyr", "randomForest", "caret", "rpart", "rpart.plot"), type = "binary")

# Load libraries
library(dplyr)
library(randomForest)
library(caret)
library(rpart)
library(rpart.plot)
```

#### i) Load and Prepare Data
```r
# Load dataset
heart_disease <- read.csv("<path>/heart_disease.csv")

# Convert relevant columns to factors
heart_disease <- heart_disease %>% 
  mutate(across(c(sex, cp, fbs, restecg, exang, slope, ca, thal, target), factor))
```

#### ii) Split Data into Training and Testing Sets
```r
set.seed(123)  # Ensure reproducibility

# Split data (70% training, 30% testing)
train_index <- 1:(nrow(heart_disease) * 0.7)
train_data <- heart_disease[train_index, ]
test_data <- heart_disease[-train_index, ]
```

#### iii) Build and Plot Decision Tree Model
```r
# Train decision tree model
tree_model <- rpart(target ~ ., data = train_data, method = "class")

# Visualize decision tree
rpart.plot(tree_model)
```

##### Inferences from Decision Tree
- **Left Node Split**: Patients with thalassemia < 3 move left; represents 45% of data.
- **Age Split**: If age < 59, branches leftward (35% of data). 
  - Probability of heart disease diagnosis: 
    - Thalassemia = 3, age < 59 → 0.08
    - Thalassemia = 3, age > 59, chest pain = 4 → 0.73
- **Right Node Split**: Splits further on chest pain and number of major vessels.
  - Probability of diagnosis increases with number of vessels:
    - Chest pain ≤ 3, vessels = 0 → 0.24
    - Chest pain ≤ 3, vessels > 0 → 0.75
    - Chest pain = 4 → 0.89 (31% of total population).

#### iv) Evaluate Decision Tree
```r
# Predict with decision tree
pred_tree <- predict(tree_model, test_data, type = "class")

# Confusion matrix for decision tree
confusion_matrix_tree <- confusionMatrix(pred_tree, test_data$target)

# Metrics
confusion_matrix_tree$overall['Accuracy']
confusion_matrix_tree$byClass[c('Precision', 'Recall')]
```

**Results**:
- **Accuracy**: 76.67%
- **Precision**: 73.58%
- **Recall**: 84.78%

---

#### v) Train Random Forest Model (mtry = 13)
```r
set.seed(123)

# Train random forest model
rf_model_1 <- randomForest(target ~ ., data = train_data, ntree = 100, mtry = 13)

# Predict with random forest
pred_rf_1 <- predict(rf_model_1, test_data)

# Confusion matrix for random forest (mtry = 13)
confusionMatrix(pred_rf_1, test_data$target)
```

**Results**:
- **Accuracy**: 80%
- **Sensitivity**: 91.30%
- **Specificity**: 68.18%

---

#### vi) Train Random Forest Model (Default mtry)
```r
set.seed(123)

# Train random forest with default mtry
rf_model_2 <- randomForest(target ~ ., data = train_data, ntree = 100)

# Predict with random forest
pred_rf_2 <- predict(rf_model_2, test_data)

# Confusion matrix for random forest (default mtry)
confusionMatrix(pred_rf_2, test_data$target)
```

**Results**:
- **Accuracy**: 81.11%
- **Sensitivity**: 89.13%
- **Specificity**: 72.73%

---

####################################################################################

 v) 
set.seed(123) 
rf_model_1 <- randomForest(target ~ age + sex + cp + trestbps + chol + fbs + restecg + thalach + exang 
+ oldpeak + slope + ca + thal,data = train_data, ntree = 100, mtry = 13) 
 
pred_rf_1 <- predict(rf_model_1, test_data) 
confusionMatrix(pred_rf_1, test_data$target) 
 
Confusion Matrix and Statistics 
 
          Reference 
Prediction  0  1 
         0 42 14 
         1  4 30 
                                           
               Accuracy : 0.8              
                 95% CI : (0.7025, 0.8769) 
    No Information Rate : 0.5111           
    P-Value [Acc > NIR] : 1.329e-08        
                                           
                  Kappa : 0.5978           
                                           
 Mcnemar's Test P-Value : 0.03389          
IAI                                                                                                                                                  CS2B-1124 
Page 5 of 19 
 
                                           
            Sensitivity : 0.9130           
            Specificity : 0.6818           
         Pos Pred Value : 0.7500           
         Neg Pred Value : 0.8824           
             Prevalence : 0.5111           
         Detection Rate : 0.4667           
   Detection Prevalence : 0.6222           
      Balanced Accuracy : 0.7974           
                                           
       'Positive' Class : 0  
(4) 

############################################################################################
 vi)  
The default value of mtry is d/3 or √𝑑   ~ 4 (rounded off) 
 
set.seed(123) 
rf_model_2 <- randomForest(target ~ age + sex + cp + trestbps + chol + fbs + restecg + thalach 
+ exang + oldpeak + slope + ca + thal,data = train_data, ntree = 100) 
 
pred_rf_2 <- predict(rf_model_2, test_data) 
confusionMatrix(pred_rf_2, test_data$target) 
 
Confusion Matrix and Statistics 
 
          Reference 
Prediction  0  1 
         0 41 12 
         1  5 32 
                                           
               Accuracy : 0.8111           
                 95% CI : (0.7149, 0.8859) 
    No Information Rate : 0.5111           
    P-Value [Acc > NIR] : 3.351e-09        
                                           
                  Kappa : 0.6207           
                                           
 Mcnemar's Test P-Value : 0.1456           
                                           
            Sensitivity : 0.8913           
            Specificity : 0.7273           
         Pos Pred Value : 0.7736           
         Neg Pred Value : 0.8649           
             Prevalence : 0.5111           
         Detection Rate : 0.4556           
   Detection Prevalence : 0.5889           
      Balanced Accuracy : 0.8093           
                                           
       'Positive' Class : 0  

##################################################################
Alternate answer 
 
Confusion Matrix and Statistics 
 

Reference 
Prediction  0  1 
0 42 13 
1  4 31 
Accuracy : 0.8111           
95% CI : (0.7149, 0.8859) 
No Information Rate : 0.5111           
P-Value [Acc > NIR] : 3.351e-09        
Kappa : 0.6203           
Mcnemar's Test P-Value : 0.05235          
Sensitivity : 0.9130           
Specificity : 0.7045           
Pos Pred Value : 0.7636           
Neg Pred Value : 0.8857           
Prevalence : 0.5111           
Detection Rate : 0.4667           
Detection Prevalence : 0.6111           
Balanced Accuracy : 0.8088           
'Positive' Class : 0                                   
Here’s a neatly organized version of your analysis for sections vii, viii, and ix:

---

### vii) Accuracy Comparison

```r
# Decision Tree Accuracy
tree_accuracy <- confusion_matrix_tree$overall['Accuracy']
cat("Decision Tree Accuracy:", tree_accuracy, "\n")

# Random Forest Accuracy (mtry = 13)
rf_accuracy_1 <- confusionMatrix(pred_rf_1, test_data$target)$overall['Accuracy']
cat("Random Forest Accuracy mtry 13:", rf_accuracy_1, "\n")

# Random Forest Accuracy (default mtry)
rf_accuracy_2 <- confusionMatrix(pred_rf_2, test_data$target)$overall['Accuracy']
cat("Random Forest Accuracy default mtry:", rf_accuracy_2, "\n")
```

**Results**:
- Decision Tree Accuracy: **0.7667**
- Random Forest Accuracy (mtry = 13): **0.8**
- Random Forest Accuracy (default mtry): **0.8111**

**Observation**:
- Random Forest models outperform the Decision Tree model in this scenario.
- The default mtry value (selecting 4 predictors at random) yields slightly higher accuracy than using all predictors (mtry = 13).
- Using all predictors corresponds to **bagging**, whereas selecting fewer predictors avoids overfitting while increasing accuracy.

---

### viii) Characteristics of Decision Trees and Random Forests

**Decision Trees**:
- Prone to high variance.
- Sensitive to data and can easily overfit.
- Capture detailed patterns, including noise, in the training data.

**Random Forest**:
- Combines predictions from multiple randomized trees.
- Reduces variance and mitigates overfitting by diluting the impact of noise and overly specific patterns.
- Incorporates randomness in feature selection and bootstrapped data sampling, ensuring diversity among trees.

---

### ix) ROC Curve Analysis

- **ROC Curve 3**:
  - Has the highest AUC (**0.99**), indicating superior model performance.
  - Strictly higher true positive rates for false positive rates at 5%, showcasing strong predictive power.
  - Represents the best model among the three based on its ability to balance sensitivity and specificity.

**Key Notes**:
- AUC of a random classifier: **0.5**.
- AUC of a perfect classifier: **1**.

---
