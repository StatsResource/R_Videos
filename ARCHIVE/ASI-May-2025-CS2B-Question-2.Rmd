This exercise provides a step-by-step guide to **K-means clustering**, a fundamental machine learning algorithm for **customer segmentation**. The goal is to group mall customers into distinct segments based on their age, annual income, and spending score.

The exercise first demonstrates a manual, a-step-by-step approach to the K-means algorithm, showing how initial clusters are assigned, centroids are calculated, and data points are reassigned based on Euclidean distance. This provides a clear, conceptual understanding of how the algorithm works.

Following the manual approach, the exercise uses R's built-in `kmeans()` function to automate the clustering process. It then uses the **Elbow Method** to determine the optimal number of clusters for the data. Finally, it visualizes the resulting clusters and compares the accuracy of different cluster numbers to find the most effective segmentation for marketing and business insights.

The exercise effectively combines a theoretical understanding of K-means with its practical implementation in R.

---

Question 2 - Solution

### K-means Clustering Analysis of Mall Customer Data

This analysis uses the **k-means algorithm** to segment mall customers into distinct groups based on their age, annual income, and spending score.

-----

### 1. Data Loading and Initial Exploration

The first step is to load the `Mall_Customers.csv` dataset and get a summary of its contents.

```r
Mall_Customers <- read.csv("<path>/Mall_customers.csv")
summary(Mall_Customers)
```

The summary output provides key statistics for each variable:

  * **CustomerID:** A unique identifier for each customer, from 1 to 200.
  * **Gender:** A character variable, indicating the gender of the customers.
  * **Age:** Customers in the dataset are between 18 and 70 years old.
  * **Annual Income (k$):** Income ranges from $15,000 to $137,000.
  * **Spending Score (1-100):** This score is relatively evenly distributed, with values ranging from 1 to 99, and a mean and median close to 50.

-----

### 2\. Feature Selection

To perform clustering, we'll only use the numerical variables: **Age**, **Annual Income**, and **Spending Score**. We'll create a new data frame for this purpose.

```r
Mall_Customers1 <- Mall_Customers[, 3:5]
head(Mall_Customers1)
```

The new data frame `Mall_Customers1` now contains only the three variables needed for the analysis.

-----

### 3\. Manual Clustering (Iteration 1)

This section demonstrates a manual, step-by-step approach to the k-means algorithm to understand how it works. We'll start by assigning each customer to one of four initial, equally-sized clusters.

```r
# Assign customers to four initial clusters (A, B, C, D)
Mall_Customers1$cluster_name1 <- c(rep("A", 50), rep("B", 50), rep("C", 50), rep("D", 50))

# Display the first few rows of each cluster
rows_to_print <- c(1:5, 51:55, 101:105, 151:155)
print(Mall_Customers1[rows_to_print, ])
```

#### Determining Cluster Centroids

Next, we'll calculate the initial centroids (the mean of each variable) for each of the four clusters.

```r
# Calculate the mean for each cluster
Age_A <- mean(Mall_Customers1$Age[Mall_Customers1$cluster_name1 == "A"])
Income_A <- mean(Mall_Customers1$Income[Mall_Customers1$cluster_name1 == "A"])
spend_score_A <- mean(Mall_Customers1$spend_score[Mall_Customers1$cluster_name1 == "A"])

Age_B <- mean(Mall_Customers1$Age[Mall_Customers1$cluster_name1 == "B"])
Income_B <- mean(Mall_Customers1$Income[Mall_Customers1$cluster_name1 == "B"])
spend_score_B <- mean(Mall_Customers1$spend_score[Mall_Customers1$cluster_name1 == "B"])

Age_C <- mean(Mall_Customers1$Age[Mall_Customers1$cluster_name1 == "C"])
Income_C <- mean(Mall_Customers1$Income[Mall_Customers1$cluster_name1 == "C"])
spend_score_C <- mean(Mall_Customers1$spend_score[Mall_Customers1$cluster_name1 == "C"])

Age_D <- mean(Mall_Customers1$Age[Mall_Customers1$cluster_name1 == "D"])
Income_D <- mean(Mall_Customers1$Income[Mall_Customers1$cluster_name1 == "D"])
spend_score_D <- mean(Mall_Customers1$spend_score[Mall_Customers1$cluster_name1 == "D"])
```

The calculated centroids are:

  * **Cluster A:** Age: 35.28, Income: 27.4, Spending Score: 49.48
  * **Cluster B:** Age: 44.22, Income: 51.72, Spending Score: 50.38
  * **Cluster C:** Age: 38.58, Income: 69.2, Spending Score: 50.98
  * **Cluster D:** Age: 37.32, Income: 93.92, Spending Score: 49.96

#### Reassigning Clusters Based on Euclidean Distance

Now we calculate the **Euclidean distance** from each customer to each of the four cluster centroids. The customer is then reassigned to the cluster with the minimum distance.

```r
# Calculate distance to each centroid
Mall_Customers1$dist_A <- sqrt((Mall_Customers1$Age - Age_A)^2 + (Mall_Customers1$Income - Income_A)^2 + (Mall_Customers1$spend_score - spend_score_A)^2)
Mall_Customers1$dist_B <- sqrt((Mall_Customers1$Age - Age_B)^2 + (Mall_Customers1$Income - Income_B)^2 + (Mall_Customers1$spend_score - spend_score_B)^2)
Mall_Customers1$dist_C <- sqrt((Mall_Customers1$Age - Age_C)^2 + (Mall_Customers1$Income - Income_C)^2 + (Mall_Customers1$spend_score - spend_score_C)^2)
Mall_Customers1$dist_D <- sqrt((Mall_Customers1$Age - Age_D)^2 + (Mall_Customers1$Income - Income_D)^2 + (Mall_Customers1$spend_score - spend_score_D)^2)

# Reassign customers to the closest cluster
Mall_Customers1$cluster_name2 <- apply(Mall_Customers1[, 5:8], 1, function(row) {
  names(Mall_Customers1[, 5:8])[which.min(row)]
})
Mall_Customers1$cluster_name2 <- stringr::str_sub(Mall_Customers1$cluster_name2, -1, -1)

# Display the reassigned clusters for the same customers
print(Mall_Customers1[rows_to_print, ])
```

The output shows that some customers have been reassigned to new clusters based on their proximity to the calculated centroids.

#### Comparing Original vs. Updated Clusters

A contingency table helps visualize how many customers changed clusters after the first iteration.

```r
table(Mall_Customers1$cluster_name1, Mall_Customers1$cluster_name2)
```

The results show the following movements:

  * 4 customers from original cluster A moved to cluster B.
  * 1 customer from original cluster B moved to cluster A, and 8 moved to cluster C.
  * 9 customers from original cluster C moved to cluster B.
  * 13 customers from original cluster D moved to cluster C.

This demonstrates that the initial random assignment was not optimal and that the customers were reassigned to clusters that better fit their data points.

-----

### 4\. Full K-means Algorithm Implementation

Instead of manually calculating centroids and distances, we can use the built-in `kmeans` function in R, which automates this process until the clusters converge and no more customers change their assignment.

```r
# Run k-means with 4 clusters
set.seed(123)
kmeans_model_4 <- stats::kmeans(Mall_Customers1[, 1:3], centers = 4, nstart = 10)
```

#### Results for 4 Clusters

The results of the automated k-means model with 4 clusters:

  * **Cluster Sizes:** 28, 39, 38, and 95 customers.
  * **Cluster Means:** The final centroids for each cluster are shown below.
      * **Cluster 1:** Age: 24.8, Income: 28.7, Spend Score: 74.3
      * **Cluster 2:** Age: 32.7, Income: 86.5, Spend Score: 82.1
      * **Cluster 3:** Age: 40.4, Income: 87.0, Spend Score: 18.6
      * **Cluster 4:** Age: 44.9, Income: 48.7, Spend Score: 42.6
  * **Explained Variance:** The `(between_SS / total_SS)` value is **66.2%**, meaning 66.2% of the total variance in the data is explained by the clustering.

#### Visualizing the 4 Clusters

We can visualize these clusters using a plot.

```r
# Create a plot with points colored by cluster
plot(Mall_Customers1[, 1:3], col = kmeans_model_4$cluster)
```

This plot (of the first two variables, Age and Income, or Income and Spend Score) will show four distinct groups of customers.

-----

### 5\. Determining the Optimal Number of Clusters

A key challenge in k-means is choosing the right number of clusters. The **Elbow Method** is a common technique to find the optimal number of clusters. We'll run the k-means algorithm for 1 to 10 clusters and plot the **total within-cluster sum of squares (WSS)** for each.

```r
set.seed(123)
ks <- 1:10
tot_within_ss <- sapply(ks, function(k) {
  c2 <- kmeans(Mall_Customers1[, 1:3], k, nstart = 10)
  c2$tot.withinss
})

plot(ks, tot_within_ss, type = "b",
     main = "Elbow Method for Optimal Clusters",
     xlab = "Number of Clusters (k)",
     ylab = "Total Within Sum of Squares")
```

The plot shows a sharp "elbow" at **$k=5$**, where the decrease in WSS begins to slow down. This suggests that **5 clusters** are the most appropriate for this dataset.

-----

### 6\. Final K-means Model with 5 Clusters

Based on the Elbow Method, we'll run the k-means algorithm again with 5 clusters to get a better-performing model.

```r
# Run k-means with 5 clusters
set.seed(123)
kmeans_model_5 <- stats::kmeans(Mall_Customers1[, 1:3], centers = 5, nstart = 10)
```

#### Results for 5 Clusters

The new model provides the following results:

  * **Cluster Sizes:** 23, 23, 79, 36, and 39 customers.
  * **Explained Variance:** The new `(between_SS / total_SS)` value is **75.6%**. This is a significant improvement over the 66.2% from the 4-cluster model, indicating a better fit.

#### Visualizing the 5 Clusters

```r
# Create a plot with points colored by the new 5-cluster assignments
plot(Mall_Customers1[, 1:3], col = kmeans_model_5$cluster)
```

